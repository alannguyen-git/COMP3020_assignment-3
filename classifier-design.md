# How does it work?

My classifier processes the refined descriptions of TV shows as its input data. The input data follows two distinct paths. The first path involves passing through an Integer Vectoriser, which serves as the initial input for a pre-trained embedding layer. The output from this embedding layer is then transformed into a 1D representation through averaging, and it is subsequently combined with the output from another TF-IDF Vectoriser. This integrated information is then learned with a two hidden layers, producing an output of 28 neurons, each corresponding to a specific genre. I used very small increment to find the best threshold that will produce the highest accuracy and F1-score.

# Why this design?

Firstly, the reason for employing two Vectorisers is to utilise the capabilities of the Embedding layer for learning the similarities and semantics within the descriptions. Additionally, the TF-IDF output provides frequency-based features. With the information harnessed from the Embedding layer and TF-IDF, I then use 2 hidden layers to learn. Most importantly is the output layer which uses sigmoid for the ease of understanding. Since this is a multi-label classification task, each output will represents the probability that the output is the correct genre. It's the way that my model is trained that is special. I first imported the structure of the model and use keras-tuner to find the best hyper-parameter. After knowing the best hyper-parameter for this dataset, I then fixed them to save time when re-trained. However, it is still important to use keras-tuner even though the best hyper-parameters are known, it is keras-tuner ability to find the best initialisation of the weights that give my model an extra 20\% accuracy. I then retrieve the best hyper-parameters and the initialisation and re-train it to find the best epoch. It seems like my model could use some regularisation since the best epoch is often low. However, whenever adding a drop-out layer, my model performance always drop dramatically so I settle with an F1-score of around 62\% which is good enough.

# NOTE
I will be downloading a pre-trained fasttext model which is roughly 7.42 Gb, this might take a while when you first run my train.py. However, the second time will be faster as you don't have to download the model again.